{
  "output": "Smaller, offshoot tech cycles happen all the time, but every once in a while — historically, about every 10 to 15 years — major new cycles begin that completely reshape the computing landscape.The PC enabled entrepreneurs to create word processors, spreadsheets, and many other desktop applications.\nIt is likely that many more mobile innovations are still to come.Each product era can be divided into two phases: 1) the gestation phase, when the new platform is first introduced but is expensive, incomplete, and/or difficult to use, 2) the growth phase, when a new product comes along that solves those problems, kicking off a period of exponential growth.The Apple II was released in 1977 (and the Altair in 1975), but it was the release of the IBM PC in 1981 that kicked off the PC growth phase.The internet’s gestation phase took place in the 80s and early 90s when it was mostly a text-based tool used by academia and government.\nIf you disassemble a modern drone, VR headset, or IoT devices, you’ll find mostly smartphone components.In the modern semiconductor era, the focus has shifted from standalone CPUs to bundles of specialized chips known as systems-on-a-chip.Typical systems-on-a-chip bundle energy-efficient ARM CPUs plus specialized chips for graphics processing, communications, power management, video processing, and more.This new architecture has dropped the price of basic computing systems from about $100 to about $10.\nNvidia’s roadmap promises significant performance improvements in the coming years.A wildcard technology is quantum computing, which today exists mostly in laboratories but if made commercially viable could lead to orders-of-magnitude performance improvements for certain classes of algorithms in fields like biology and artificial intelligence.There are many exciting things happening in software today.\nInteresting distributed systems technologies include systems like Hadoop and Spark for parallelizing big data problems, and Bitcoin/blockchain for securing data and assets.But perhaps the most exciting software breakthroughs are happening in artificial intelligence (AI).\nHowever, there are good reasons to think that AI might now finally be entering a golden age.“Machine learning is a core, transformative way by which we’re rethinking everything we’re doing.” — Google CEO, Sundar PichaiA lot of the excitement in AI has focused on deep learning, a machine learning technique that was popularized by a now famous 2012 Google project that used a giant cluster of computers to learn to identify cats in YouTube videos.\nIt was brought back to life by a combination of factors, including new algorithms, cheap parallel computation, and the widespread availability of large data sets.It’s tempting to dismiss deep learning as another Silicon Valley buzzword.\nSoftware tools like Theano and TensorFlow, combined with cloud data centers for training, and inexpensive GPUs for deployment, allow small teams of engineers to build state-of-the-art AI systems.For example, here a solo programmer working on a side project used TensorFlow to colorize black-and-white photos:And here a small startup created a real-time object classifier:Which of course is reminiscent of a famous scene from a sci-fi movie:One of the first applications of deep learning released by a big tech company is the search function in Google Photos, which is shockingly smart.We’ll soon see significant upgrades to the intelligence of all sorts of products, including: voice assistants, search engines, chat bots, 3D scanners, language translators, automobiles, drones, medical imaging systems, and much more.“The business plans of the next 10,000 startups are easy to forecast: Take X and add AI.\nThis is a big deal, and now it’s here.” — Kevin KellyStartups building AI products will need to stay laser focused on specific applications to compete against the big tech companies who have made AI a top priority.\nAI systems get better as more data is collected, which means it’s possible to create a virtuous flywheel of data network effects (more users → more data → better products → more users).\nSuccessful AI startups will follow a similar strategy.There are a variety of new computing platforms currently in the gestation phase that will soon get much better — and possibly enter the growth phase — as they incorporate recent advances in hardware and software.\nDeep learning software tools have gotten so good that a solo programmer was able to make a semi-autonomous car:Drones.\nLanguage understanding should improve quickly as recent breakthroughs in deep learning make their way into production devices.IoT will also be adopted in business contexts.\nMajor areas of research will include: 1) new tools for creating rendered and/or filmed VR content, 2) machine vision for tracking and scanning directly from phones and headsets, and 3) distributed back-end systems for hosting large virtual environments.Augmented Reality.\nThis demo video was shot directly through Magic Leap’s AR device:It is possible that the pattern of 10–15 year computing cycles has ended and mobile is the final era.\nIt is also possible the next era won’t arrive for a while, or that only a subset of the new computing categories discussed above will end up being important.I tend to think we are on the cusp of not one but multiple new eras.\nThe “peace dividend of the smartphone war” created a Cambrian explosion of new devices, and developments in software, especially AI, will make those devices smart and useful.",
  "id": "19fe5fe2-89a1-48f5-b1ab-8efd448caaff"
}